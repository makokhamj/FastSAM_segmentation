{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3de9646a-7e8a-46f7-86e7-ddfabacc21ba",
   "metadata": {},
   "source": [
    "IMPLEMENTING IMAGE SEGMENTATION, COUNTING PEOPLE AND SHOWING BOUNDING BOXES\n",
    "\n",
    "NOTE: \n",
    "FastSAM (or most “segment anything” models) does not always segment “people only” out of the box.\n",
    "By default, these models generate masks for all salient objects, not just people.\n",
    "\n",
    "Why Does This Happen?\n",
    "\t•\tFastSAM and similar models are category-agnostic by default: they segment everything that looks like an object.\n",
    "\t•\tThe mask(s) you get often include people, but also objects, animals, backgrounds, etc.\n",
    "\n",
    "\n",
    "How to Remove Everything Except People\n",
    "\n",
    "1. Use a People/Person Class Mask\n",
    "\t•\tYou need a model that knows what a “person” is. This is called a class-aware or semantic segmentation or detection model.\n",
    "\t•\tOption 1: Use a dedicated human segmentation model or person detector.\n",
    "\t•\tOption 2: Filter FastSAM’s masks using a person detector (e.g., YOLO, Faster R-CNN, or segment_anything + detectron2/GroundingDINO).\n",
    "\n",
    "\n",
    "2. Practical Solution: Two-Stage Pipeline\n",
    "\n",
    "A. Detect people first (bounding boxes), then use segmentation mask\n",
    "\t1.\tRun an object detector (like YOLOv8, YOLOv5, or any with a “person” class) to get bounding boxes for people.\n",
    "\t2.\tFor each mask from FastSAM, check if its bounding box overlaps with a detected “person” box.\n",
    "\t•\tIf yes, keep the mask.\n",
    "\t•\tIf no, discard the mask.\n",
    "\t3.\tCombine only the “person” masks to create your final mask.\n",
    "\n",
    "B. Use a people-specific segmentation model (recommended if only people matter)\n",
    "\t•\tModels like DeepLabv3+ with “person” class or Selfie Segmentation from MediaPipe are designed for this.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86a56f87-e62e-4603-b54d-e4fa66a51112",
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOU DO NOT NEED ALL THESE PACKAGES, BUT I DO BECAUSE OF ALL THE OTHER LITTLE STEPS I WANTED TO RUN\n",
    "# I AM RUNNING python==3.10.15\n",
    "\n",
    "import os\n",
    "import cv2\n",
    "import torch\n",
    "import roboflow\n",
    "from roboflow import Roboflow\n",
    "import base64\n",
    "import supervision as sv\n",
    "import numpy as np\n",
    "from fastsam import FastSAM, FastSAMPrompt\n",
    "from segment_anything import sam_model_registry, SamAutomaticMaskGenerator, SamPredictor\n",
    "import random\n",
    "\n",
    "from matplotlib.backends.backend_agg import FigureCanvasAgg\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from ultralytics.nn.tasks import SegmentationModel\n",
    "from ultralytics import YOLO\n",
    "\n",
    "print(\"DONE\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d6a1805-3340-474b-8229-404d288d17d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#LOAD FAST_SAM MODEL\n",
    "\n",
    "DEVICE = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"DEVICE = {DEVICE}\")\n",
    "fast_sam = FastSAM(\"FastSAM.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afa2d642-a004-4114-aedb-e1b912b342b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SELECT AND FORMAT PATHS TO FILE TO BE USED, AS WELL AS NAME/LOCATION TO BE SAVED\n",
    "\n",
    "file_n = \"construction3.jpg\" # This is the image to be worked on\n",
    "file2_n = \"constr3\"      # Name of generated file to be saved\n",
    "img_r = \"./images/\"     # Path to image for use\n",
    "IMAGE_PATH = f\"{img_r}{file_n}\"\n",
    "output_p = f\"{'./output'}{'/'}{file2_n}{'.jpg'}\"\n",
    "print(IMAGE_PATH, output_p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11bcd5c3-7fd2-44dd-9737-1aae5e59f00f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the FastSAM segmentation model on the image located at IMAGE_PATH\n",
    "\n",
    "results = fast_sam(\n",
    "    source=IMAGE_PATH,\n",
    "    device=DEVICE,\n",
    "    retina_masks=True,\n",
    "    imgsz=1024,\n",
    "    conf=0.4,\n",
    "    iou=0.9)\n",
    "prompt_process = FastSAMPrompt(IMAGE_PATH, results, device=DEVICE) # Initializes a helper object (FastSAMPrompt) to further process or\n",
    "                                                                   # interact with the masks/predictions generated by fast_sam\n",
    "fastsam_masks = prompt_process.everything_prompt() # Extracts all masks (i.e., every segmented object) from the image\n",
    "prompt_process.plot(annotations=fastsam_masks, output_path=f\"{'./output/'}{file2_n}{'.jpg'}\")\n",
    "print(\"done with this\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1aef150-307d-40a3-922a-fc77b679e587",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converts masks to boolean (True/False)\n",
    "\n",
    "def masks_to_bool(masks):\n",
    "    if type(masks) == np.ndarray:\n",
    "        return masks.astype(bool)\n",
    "    return masks.cpu().numpy().astype(bool)\n",
    "print(\"done with this too\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7b1946b-b291-4b48-aada-05569d949584",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Takes an image file path (image_path) and a set of segmentation masks (masks), and returns a NumPy array (the annotated image).\n",
    "\n",
    "def annotate_image(image_path: str, masks: np.ndarray) -> np.ndarray:\n",
    "    import numpy as np\n",
    "    image = cv2.imread(image_path)\n",
    "    masks = np.array(masks).astype(bool)  # Ensure masks is a NumPy array\n",
    "    xyxy = sv.mask_to_xyxy(masks=masks)\n",
    "    detections = sv.Detections(xyxy=xyxy, mask=masks)\n",
    "    mask_annotator = sv.MaskAnnotator(color_lookup = sv.ColorLookup.INDEX)\n",
    "    return mask_annotator.annotate(scene=image.copy(), detections=detections)\n",
    "print(\"done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67597148-a906-4c80-a5e3-590709e3fc21",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mask and annotate\n",
    "\n",
    "masks = masks_to_bool(fastsam_masks)\n",
    "annotated_image=annotate_image(image_path=IMAGE_PATH, masks=masks)\n",
    "sv.plot_image(image=annotated_image, size=(6, 4))\n",
    "print('done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b504926-6cac-4141-915d-e7d68628c781",
   "metadata": {},
   "outputs": [],
   "source": [
    "# REMOVE BACKGROUND AND THEN REPLACE WITH WHITE\n",
    "\n",
    "def remove_background(image_path: str, mask: np.ndarray) -> np.ndarray:\n",
    "    image = cv2.imread(image_path, cv2.IMREAD_UNCHANGED)\n",
    "    h, w = image.shape[:2]\n",
    "    # If mask is not same size as image, resize\n",
    "    mask = cv2.resize(mask.astype(np.uint8), (w, h), interpolation=cv2.INTER_NEAREST)\n",
    "\n",
    "    # If mask has multiple objects, combine into single mask\n",
    "    if len(mask.shape) > 2:\n",
    "        mask = np.any(mask, axis=0).astype(np.uint8)  # [num_masks, H, W] -> [H, W]\n",
    "    \n",
    "    # Create alpha channel: foreground (object) = 255, background = 0\n",
    "    alpha = (mask * 255).astype(np.uint8)\n",
    "    # Add alpha channel to the image\n",
    "    bgr = image[..., :3]\n",
    "    rgba = cv2.cvtColor(bgr, cv2.COLOR_BGR2BGRA)\n",
    "    rgba[..., 3] = alpha\n",
    "    return rgba\n",
    "\n",
    "final_mask = np.any(masks, axis=0).astype(np.uint8) if masks.ndim == 3 else masks\n",
    "\n",
    "rgba_result = remove_background(IMAGE_PATH, final_mask)\n",
    "#cv2.imwrite(output_p, rgba_result)\n",
    "\n",
    "#sv.plot_image(rgba_result, size=(8, 8))\n",
    "\n",
    "def remove_background_white(image_path: str, mask) -> np.ndarray:\n",
    "    import numpy as np\n",
    "    import cv2\n",
    "\n",
    "    # Convert torch tensor to numpy if needed\n",
    "    if hasattr(mask, 'detach'):\n",
    "        mask = mask.detach().cpu().numpy()\n",
    "\n",
    "    image = cv2.imread(image_path)\n",
    "    h, w = image.shape[:2]\n",
    "\n",
    "    # If mask has multiple objects, collapse to a single mask\n",
    "    if mask.ndim > 2:\n",
    "        mask = np.any(mask, axis=0).astype(np.uint8)\n",
    "\n",
    "    # Resize mask to match image (width, height)\n",
    "    mask = cv2.resize(mask.astype(np.uint8), (w, h), interpolation=cv2.INTER_NEAREST)\n",
    "\n",
    "    # Broadcast mask to 3 channels if needed\n",
    "    mask_3ch = np.repeat(mask[:, :, np.newaxis], 3, axis=2)\n",
    "\n",
    "    white_bg = np.ones_like(image) * 255\n",
    "    result = np.where(mask_3ch == 1, image, white_bg)\n",
    "    return result\n",
    "\n",
    "whiteBG = remove_background_white(IMAGE_PATH, masks)\n",
    "#cv2.imwrite(output_p, whiteBG)\n",
    "\n",
    "#sv.plot_image(whiteBG, size=(6, 4))\n",
    "print('done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1dfb623-5b63-46ae-8a6d-0c719f5058e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code Outline: FastSAM + YOLOv8 for “person” filtering\n",
    "\n",
    "# Step 1. Detect people with YOLO\n",
    "\n",
    "# Load YOLOv8 or v5 model\n",
    "yolo_model = YOLO(\"yolov8n.pt\")  # or yolov5s.pt if you prefer\n",
    "\n",
    "results = yolo_model(IMAGE_PATH)\n",
    "person_bboxes = []\n",
    "for box, cls in zip(results[0].boxes.xyxy.cpu().numpy(), results[0].boxes.cls.cpu().numpy()):\n",
    "    if int(cls) == 0:  # class 0 is 'person' in COCO\n",
    "        person_bboxes.append(box)  # [x1, y1, x2, y2]\n",
    "\n",
    "print(\"done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e85beac9-0be0-43a3-86fb-3ea95bbe0838",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2. Filter FastSAM masks\n",
    "\n",
    "def bbox_overlap(mask_bbox, person_bboxes, iou_threshold=0.2):\n",
    "    # mask_bbox: [x1, y1, x2, y2]\n",
    "    # person_bboxes: list of [x1, y1, x2, y2]\n",
    "    xA = np.maximum(mask_bbox[0], [b[0] for b in person_bboxes])\n",
    "    yA = np.maximum(mask_bbox[1], [b[1] for b in person_bboxes])\n",
    "    xB = np.minimum(mask_bbox[2], [b[2] for b in person_bboxes])\n",
    "    yB = np.minimum(mask_bbox[3], [b[3] for b in person_bboxes])\n",
    "    interArea = np.maximum(0, xB - xA + 1) * np.maximum(0, yB - yA + 1)\n",
    "    maskArea = (mask_bbox[2] - mask_bbox[0] + 1) * (mask_bbox[3] - mask_bbox[1] + 1)\n",
    "    personAreas = [(b[2] - b[0] + 1) * (b[3] - b[1] + 1) for b in person_bboxes]\n",
    "    iou = interArea / (maskArea + np.array(personAreas) - interArea + 1e-6)\n",
    "    return np.any(iou > iou_threshold)\n",
    "\n",
    "# Filter masks\n",
    "def compute_bbox_from_mask(mask):\n",
    "    import numpy as np\n",
    "    ys, xs = np.where(mask > 0)\n",
    "    if len(xs) == 0 or len(ys) == 0:\n",
    "        return [0, 0, 0, 0]\n",
    "    x1, y1, x2, y2 = xs.min(), ys.min(), xs.max(), ys.max()\n",
    "    return [x1, y1, x2, y2]\n",
    "\n",
    "person_masks = []\n",
    "for i, mask in enumerate(fastsam_masks):  # fastsam_masks: shape (N, H, W)\n",
    "    mask_bbox = compute_bbox_from_mask(mask)\n",
    "    if bbox_overlap(mask_bbox, person_bboxes):\n",
    "        person_masks.append(mask)\n",
    "\n",
    "print(\"done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba93a9b7-5d81-414d-8553-ab9db96df0fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the bounding boxes and masks corresponding to people\n",
    "#\t•\tDraws each YOLO person bounding box in red.\n",
    "#\t•\tOverlays each mask classified as “human” in a translucent color (blue or random).\n",
    "#\t•\tDraws a cyan dashed bounding box around each accepted mask for clarity.\n",
    "\n",
    "def draw_bboxes_and_masks(image_path, person_bboxes, person_masks):\n",
    "    import cv2\n",
    "    import numpy as np\n",
    "    import matplotlib.pyplot as plt\n",
    "    import random\n",
    "\n",
    "    image = cv2.imread(image_path)\n",
    "    image_rgb = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "    plt.figure(figsize=(10, 10))\n",
    "    plt.imshow(image_rgb)\n",
    "    ax = plt.gca()\n",
    "\n",
    "    # Draw YOLO person bboxes in red\n",
    "    for box in person_bboxes:\n",
    "        x1, y1, x2, y2 = box\n",
    "        rect = plt.Rectangle((x1, y1), x2 - x1, y2 - y1, linewidth=2, edgecolor='red', facecolor='none')\n",
    "        ax.add_patch(rect)\n",
    "\n",
    "    h, w = image.shape[:2]\n",
    "    overlay = np.zeros((h, w, 3), dtype=np.uint8)\n",
    "    for i, mask in enumerate(person_masks):\n",
    "        # --- Fix: Convert torch tensor to numpy if needed ---\n",
    "        if hasattr(mask, 'detach'):\n",
    "            mask = mask.detach().cpu().numpy()\n",
    "        mask = mask.astype(bool)\n",
    "        color = (random.randint(0,255), random.randint(0,255), 255)\n",
    "        overlay[mask] = color[:3]\n",
    "        x1, y1, x2, y2 = compute_bbox_from_mask(mask)\n",
    "        rect = plt.Rectangle((x1, y1), x2 - x1, y2 - y1, linewidth=2, edgecolor='cyan', facecolor='none', linestyle='--')\n",
    "        ax.add_patch(rect)\n",
    "\n",
    "    plt.imshow(overlay, alpha=0.35)\n",
    "    plt.title(\"People Bounding Boxes (red) and Accepted Masks (blue/cyan)\")\n",
    "    plt.axis('off')\n",
    "    plt.show()\n",
    "\n",
    "# Example usage:\n",
    "draw_bboxes_and_masks(IMAGE_PATH, person_bboxes, person_masks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81748505-33b9-44a2-8220-3606693c2142",
   "metadata": {},
   "outputs": [],
   "source": [
    "# A. For Images With or Without Alpha Channel (General Case)\n",
    "\n",
    "def show_image(image, title=\"Result\"):\n",
    "    # If you read with OpenCV, image is BGR; convert to RGB for matplotlib\n",
    "    if image.shape[2] == 4:  # RGBA image\n",
    "        image_rgb = cv2.cvtColor(image, cv2.COLOR_BGRA2RGBA)\n",
    "    else:  # RGB or BGR image\n",
    "        image_rgb = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "    plt.figure(figsize=(6, 4))\n",
    "    plt.imshow(image_rgb)\n",
    "    plt.axis('off')\n",
    "    plt.title(title)\n",
    "    plt.show()\n",
    "\n",
    "# Usage\n",
    "annotated_img = annotate_image(IMAGE_PATH, np.array(person_masks))\n",
    "show_image(annotated_img, \"People Segmentation Result\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3c008bb-702d-47a8-8556-512ec3ac5a88",
   "metadata": {},
   "outputs": [],
   "source": [
    "# B. For White Background Results Only (3-channel BGR image)\n",
    "\n",
    "plt.figure(figsize=(6, 6))\n",
    "plt.imshow(cv2.cvtColor(whiteBG, cv2.COLOR_BGR2RGB))\n",
    "plt.axis('off')\n",
    "plt.title(\"People Segmentation Result\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1095b15-2427-4f0e-bd7e-f4c5768d2b0f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
